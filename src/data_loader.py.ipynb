{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dba6c7c8-4453-46f4-9e0f-519d27d095a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, spark: SparkSession, config: dict):\n",
    "        self.spark = spark\n",
    "        self.config = config\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load all CSV files specified in the configuration file.\n",
    "        :return: Dictionary of DataFrames, each loaded from a CSV file.\n",
    "        \"\"\"\n",
    "        data_frames = {}\n",
    "        for file_name in self.config['input']['file_names']:\n",
    "            file_path = os.path.join(self.config['input']['data_path'], file_name)\n",
    "            index=file_name.split('.')[0]\n",
    "            data_frames[index] = self.spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        return data_frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e2b8315-9a2b-4644-9f2c-2937156c2ff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "data_loader.py",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
