{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18d3d482-e4f3-4ba5-a2a4-b75285dd5f56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('analytics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed88ccd7-fba9-43d1-8fb9-12c1f32ca899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,when,sum,count,row_number,dense_rank,countDistinct\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0b34adf-8a2e-413d-a009-9737105a7493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Analytics:\n",
    "    def __init__(self,spark,config,data_frames):\n",
    "        self.spark=spark\n",
    "        self.config=config\n",
    "        self.data_frames=data_frames\n",
    "        self.primary_people_df=self.data_frames['Primary_Person_use']\n",
    "        self.charges_df=self.data_frames['Charges_use']\n",
    "        self.units_df=self.data_frames['Units_use']\n",
    "        self.retrict_df=self.data_frames['Restrict_use']\n",
    "        self.endorse_df=self.data_frames['Endorse_use']\n",
    "        self.damage_df=self.data_frames['Damages_use']\n",
    "    def analytics_1(self):\n",
    "        \"\"\"\n",
    "        Find the number of crashes (accidents) in which number of males killed are greater than 2\n",
    "        \"\"\"\n",
    "        primary_people_df=self.data_frames['Charges_use']\n",
    "        \n",
    "        analytics_1_df=self.primary_people_df.groupBy('CRASH_ID').agg(sum(when(((col('PRSN_GNDR_ID')=='MALE')),col('DEATH_CNT')).otherwise(0)).alias('total_death')).filter(col('total_death')>2)\n",
    "        self.save(analytics_1_df,'analysis_1')\n",
    "        return analytics_1_df\n",
    "    def analytics_2(self):\n",
    "        \"\"\"\n",
    "        How many two wheelers are booked for crashes\n",
    "        \"\"\"\n",
    "        analytics_2_df=self.units_df.filter(col('VEH_BODY_STYL_ID')=='MOTORCYCLE').select('VIN').distinct().count()\n",
    "        schema=['crashed_2_wheeler']\n",
    "        result=spark.createDataFrame([(analytics_2_df,)],schema)\n",
    "        self.save(result,'analysis_2')\n",
    "        return result\n",
    "    def analytics_3(self):\n",
    "        \"\"\"\n",
    "        Determine the Top 5 Vehicle Makes of the cars present in the crashes in which driver died and Airbags did not deploy.\n",
    "        \"\"\"\n",
    "        driver_airbag=self.primary_people_df.filter((col('PRSN_TYPE_ID')=='DRIVER') & (col('PRSN_AIRBAG_ID')=='NOT DEPLOYED') & (col('PRSN_INJRY_SEV_ID')=='KILLED')).select('CRASH_ID','PRSN_TYPE_ID','PRSN_INJRY_SEV_ID','PRSN_AIRBAG_ID','DEATH_CNT')\n",
    "\n",
    "        body_style=self.units_df.select('CRASH_ID','UNIT_DESC_ID','VEH_BODY_STYL_ID','VEH_MAKE_ID','VEH_MOD_ID').filter(col('VEH_BODY_STYL_ID').like('%CAR%'))\n",
    "        cars_joined=driver_airbag.join(body_style,driver_airbag.CRASH_ID==body_style.CRASH_ID,how='inner')\n",
    "        \n",
    "        analytics_3_df=cars_joined.groupBy('VEH_MAKE_ID').agg(sum('DEATH_CNT').alias('total_death')).orderBy(col('total_death'),ascending=False).limit(5)\n",
    "        self.save(analytics_3_df,'analysis_3')\n",
    "        return analytics_3_df\n",
    "    def analytics_4(self):\n",
    "        \"\"\"\n",
    "        Determine number of Vehicles with driver having valid licences involved in hit and run\n",
    "\n",
    "        \"\"\"\n",
    "        valid_license=self.primary_people_df.select('CRASH_ID','DRVR_LIC_TYPE_ID').filter((col('PRSN_TYPE_ID')=='DRIVER')&(col('DRVR_LIC_TYPE_ID').like('%DRIVER LIC%'))).distinct()\n",
    "\n",
    "        hit_run=self.units_df.filter(col('VEH_HNR_FL')=='Y').select('CRASH_ID','VIN','VEH_HNR_FL')\n",
    "        # hit_run.display()\n",
    "        analytics_4_df=hit_run.join(valid_license,valid_license.CRASH_ID==hit_run.CRASH_ID,how='inner').select('VIN').distinct().count()\n",
    "        schema=['vechicle_HNR_valid_licence']\n",
    "        result=spark.createDataFrame([(analytics_4_df,)],schema)\n",
    "        self.save(result,'analysis_4')\n",
    "        return result\n",
    "    def analytics_5(self):\n",
    "        \"\"\"\n",
    "        Which state has highest number of accidents in which females are not involved? \n",
    "        \"\"\"\n",
    "\n",
    "        # There is a chances that in one crashId, male and female both are invloved,so we have to remove those accidents also,so that we can have only those accident in which females are invloves\n",
    "        # This primary_people table is based on people.In this, multiple crashId could be there in rows,becasuse in one accident is assigned with one crashId and in one accidents ,multiple people could be involved.\n",
    "        female=self.primary_people_df.filter(col('PRSN_GNDR_ID')=='FEMALE').select('CRASH_ID').distinct()\n",
    "        # Use left anti join to exclude crashes involving females\n",
    "        filtered_df = self.primary_people_df.join(female, on='CRASH_ID', how='left_anti')\n",
    "        # filtered_df.display()\n",
    "        analytics_5_df=filtered_df.groupBy('DRVR_LIC_STATE_ID').agg(countDistinct('CRASH_ID').alias('total_accidents')).orderBy('total_accidents',ascending=False).limit(1).select('DRVR_LIC_STATE_ID')\n",
    "        self.save(analytics_5_df,'analysis_5')\n",
    "        return analytics_5_df\n",
    "    def analytics_6(self):\n",
    "        \"\"\"\n",
    "        Which are the Top 3rd to 5th VEH_MAKE_IDs that contribute to a largest number of injuries including death\n",
    "        \"\"\"\n",
    "        window=Window.orderBy(col('crash_sum').desc())\n",
    "        total_count_df=self.units_df.withColumn('total_count',(col('TOT_INJRY_CNT')+col('DEATH_CNT')))\n",
    "        crash_sum=total_count_df.groupBy('VEH_MAKE_ID').agg(sum('total_count').alias('crash_sum'))\n",
    "\n",
    "        top_3_5=crash_sum.withColumn('rnk',row_number().over(window))\n",
    "        analytics_6_df=top_3_5.filter(((col('rnk')>=3) & (col('rnk')<=5)))\n",
    "        self.save(analytics_6_df,'analysis_6')\n",
    "        return analytics_6_df\n",
    "    def analytics_7(self):\n",
    "        \"\"\"\n",
    "        For all the body styles involved in crashes, mention the top ethnic user group of each unique body styleÂ  \n",
    "\n",
    "        \"\"\"\n",
    "        df1=self.units_df.select('CRASH_ID','VEH_BODY_STYL_ID')\n",
    "        df2=self.primary_people_df.select('CRASH_ID','PRSN_ETHNICITY_ID')\n",
    "        join=df1.join(df2,df1.CRASH_ID==df2.CRASH_ID,how='inner')\n",
    "        # join.display()\n",
    "        df_counts = join.groupBy(\"VEH_BODY_STYL_ID\", \"PRSN_ETHNICITY_ID\").agg(count(\"*\").alias(\"total_count\"))\n",
    "        window_spec = Window.partitionBy(\"VEH_BODY_STYL_ID\").orderBy(col(\"total_count\").desc())\n",
    "\n",
    "        # Add a rank or row number column based on the window spec\n",
    "        df_ranked = df_counts.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "        # Filter for the top record within each VEH_BODY_STYL_ID if needed\n",
    "        analytics_7_df = df_ranked.filter(col(\"rank\") == 1).select('VEH_BODY_STYL_ID','PRSN_ETHNICITY_ID')\n",
    "        self.save(analytics_7_df,'analysis_7')\n",
    "        return analytics_7_df\n",
    "    def analytics_8(self):\n",
    "        \"\"\"\n",
    "        Among the crashed cars, what are the Top 5 Zip Codes with highest number crashes with alcohols as the contributing factor to a crash (Use Driver Zip Code)\n",
    "\n",
    "        \"\"\"\n",
    "        alcohols_df=self.units_df.filter(( (col('CONTRIB_FACTR_1_ID').like('%ALCOHOL%') )| (col('CONTRIB_FACTR_2_ID').like('%ALCOHOL%')) | (col('CONTRIB_FACTR_P1_ID').like('%ALCOHOL%')) ) & (col('VEH_BODY_STYL_ID').like('%CAR%'))).select('CRASH_ID','CONTRIB_FACTR_1_ID','VEH_BODY_STYL_ID').distinct()\n",
    "        driver=self.primary_people_df.select('CRASH_ID','DRVR_ZIP').distinct()\n",
    "        alco_driver=alcohols_df.join(driver,driver.CRASH_ID==alcohols_df.CRASH_ID,how='inner').select(alcohols_df['CRASH_ID'].alias('CRASH_ID'), driver['DRVR_ZIP'],'CONTRIB_FACTR_1_ID','VEH_BODY_STYL_ID')\n",
    "        analytics_8_df=alco_driver.groupBy('DRVR_ZIP').agg(countDistinct('CRASH_ID').alias('crash_count')).orderBy(col('crash_count').desc()).filter(col('DRVR_ZIP').isNotNull()).limit(5)\n",
    "        self.save(analytics_8_df,'analysis_8')\n",
    "        return analytics_8_df\n",
    "    def analytics_9(self):\n",
    "        \"\"\"\n",
    "        Count of Distinct Crash IDs where No Damaged Property was observed and Damage Level (VEH_DMAG_SCL~) is above 4 and car avails Insurance\n",
    "        \"\"\"\n",
    "        damage_9=self.damage_df.filter(col('DAMAGED_PROPERTY').like('%NO DAMAGE%')).select('CRASH_ID')\n",
    "        # damage_9.display()\n",
    "\n",
    "        \n",
    "        unit_filter=self.units_df.select('CRASH_ID','FIN_RESP_TYPE_ID','VEH_DMAG_SCL_1_ID').filter((self.units_df.VEH_DMAG_SCL_1_ID.isin('DAMAGED 6','DAMAGED 5')) & (col('FIN_RESP_TYPE_ID').like('%INSURANCE%')) )\n",
    "        analytics_9_df=unit_filter.join(damage_9,on='CRASH_ID',how='left_semi').select('CRASH_ID').distinct().count()\n",
    "\n",
    "        schema=['crashID_count']\n",
    "        result=spark.createDataFrame([(analytics_9_df,)],schema)\n",
    "        self.save(result,'analysis_9')\n",
    "        return result\n",
    "    def analytics_10(self):\n",
    "        \"\"\"\n",
    "        Determine the Top 5 Vehicle Makes where drivers are charged with speeding related offences, has licensed Drivers, used top 10 used vehicle colours and has car licensed with the Top 25 states with highest number of offences (to be deduced from the data)\n",
    "        \"\"\"\n",
    "        top_state_df=self.primary_people_df.groupBy('DRVR_LIC_STATE_ID').agg(countDistinct('CRASH_ID').alias('total_accidents')).orderBy('total_accidents',ascending=False).limit(25).select('DRVR_LIC_STATE_ID').filter(~col('DRVR_LIC_STATE_ID').isin('NA','Unknown','Other'))\n",
    "        top_state = [row['DRVR_LIC_STATE_ID'] for row in top_state_df.collect()] \n",
    "\n",
    "\n",
    "        license_state_10=self.primary_people_df.select('CRASH_ID','DRVR_LIC_TYPE_ID','DRVR_LIC_STATE_ID').filter((col('PRSN_TYPE_ID')=='DRIVER')&(col('DRVR_LIC_TYPE_ID').like('%DRIVER LIC%')) & (col('DRVR_LIC_STATE_ID').isin(top_state))  ).distinct()\n",
    "        # license_state_10.display()\n",
    "\n",
    "        top_color=self.units_df.groupBy('VEH_COLOR_ID').agg(count('*').alias('color_count')).orderBy(col('color_count').desc()).limit(10).select('VEH_COLOR_ID').distinct()\n",
    "        top_colors = [row['VEH_COLOR_ID'] for row in top_color.collect()] \n",
    "\n",
    "        speeding_color=self.units_df.filter(( (col('CONTRIB_FACTR_1_ID').like('%SPEED%') )| (col('CONTRIB_FACTR_2_ID').like('%SPEED%')) | (col('CONTRIB_FACTR_P1_ID').like('%SPEED%')) ) & (col('VEH_COLOR_ID').isin(top_colors))  ).select('CRASH_ID','CONTRIB_FACTR_1_ID','VEH_COLOR_ID','VEH_MAKE_ID')\n",
    "        # speeding_color.display()\n",
    "\n",
    "        analytics_10_df=speeding_color.join(license_state_10,on='CRASH_ID').groupBy('VEH_MAKE_ID').agg(count('*').alias('cnt')).orderBy(col('cnt').desc()).limit(5).select('VEH_MAKE_ID')\n",
    "        self.save(analytics_10_df,'analysis_10')\n",
    "        return analytics_10_df\n",
    "    def save(self,result,file_name):\n",
    "        \"\"\"save the output\n",
    "        \"\"\"\n",
    "        result_path = os.path.join(self.config['output']['result_path'], f\"{file_name}.txt\")\n",
    "        result.write.mode('overwrite').option('header',True).csv(result_path)\n",
    "\n",
    "    def run_all_analytics(self):\n",
    "        \"\"\"\n",
    "        run all analytics\n",
    "        \"\"\"\n",
    "        self.analytics_1()\n",
    "        self.analytics_2()\n",
    "        self.analytics_3()\n",
    "        self.analytics_4()\n",
    "        self.analytics_5()\n",
    "        self.analytics_6()\n",
    "        self.analytics_7()\n",
    "        self.analytics_8()\n",
    "        self.analytics_9()\n",
    "        self.analytics_10()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a82f3329-db13-4760-ab57-5e30e44c01f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "analytics.py",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
